{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96342595",
   "metadata": {},
   "source": [
    "# Final Report Generator\n",
    "## Waste Classification Model Comparison\n",
    "\n",
    "**Purpose:** Aggregate and compare results from all trained models\n",
    "\n",
    "**Models Evaluated:**\n",
    "- EfficientNetB2 (TensorFlow)\n",
    "- ConvNeXt-Tiny (PyTorch)\n",
    "- ViT-B16 (PyTorch)\n",
    "\n",
    "**Experiments:**\n",
    "- Preprocessed Dataset\n",
    "- Raw Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e2330",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db363f51",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Results from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final_results.csv\n",
    "results_file = 'final_results.csv'\n",
    "\n",
    "if not os.path.exists(results_file):\n",
    "    print(f\"❌ Error: {results_file} not found!\")\n",
    "    print(\"Please run the model training notebooks first to generate results.\")\n",
    "else:\n",
    "    df = pd.read_csv(results_file)\n",
    "    print(f\"✓ Loaded {len(df)} results from {results_file}\\n\")\n",
    "    print(\"Preview of raw data:\")\n",
    "    display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a1dfe",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Processing and Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string columns to float\n",
    "numeric_cols = ['accuracy', 'precision', 'recall', 'f1']\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Get the latest results for each model-experiment combination\n",
    "df_latest = df.sort_values('timestamp').groupby(['model_name', 'experiment_type']).tail(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LATEST RESULTS FOR EACH MODEL AND EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "display(df_latest[['model_name', 'experiment_type', 'accuracy', 'precision', 'recall', 'f1', 'loss', 'timestamp']])\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "summary_stats = df_latest.groupby('model_name')[numeric_cols].agg(['mean', 'std', 'min', 'max'])\n",
    "display(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dd287",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Comparison Table\n",
    "Compare all models across all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for better visualization\n",
    "comparison_table = df_latest.pivot_table(\n",
    "    index='model_name',\n",
    "    columns='experiment_type',\n",
    "    values=['accuracy', 'precision', 'recall', 'f1']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON: PREPROCESSED vs RAW DATASETS\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_table.round(4))\n",
    "\n",
    "# Calculate improvement from Raw to Preprocessed\n",
    "if 'Preprocessed' in df_latest['experiment_type'].values and 'Raw' in df_latest['experiment_type'].values:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IMPROVEMENT: PREPROCESSED vs RAW (Percentage Points)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model in df_latest['model_name'].unique():\n",
    "        prep_data = df_latest[(df_latest['model_name'] == model) & (df_latest['experiment_type'] == 'Preprocessed')]\n",
    "        raw_data = df_latest[(df_latest['model_name'] == model) & (df_latest['experiment_type'] == 'Raw')]\n",
    "        \n",
    "        if not prep_data.empty and not raw_data.empty:\n",
    "            print(f\"\\n{model}:\")\n",
    "            for metric in numeric_cols:\n",
    "                prep_val = prep_data[metric].values[0]\n",
    "                raw_val = raw_data[metric].values[0]\n",
    "                improvement = (prep_val - raw_val) * 100\n",
    "                print(f\"  {metric.capitalize():12s}: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddf6b6",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Best Performing Models\n",
    "Identify top performers for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfc1c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST PERFORMING MODELS BY METRIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for metric in numeric_cols:\n",
    "    best_overall = df_latest.loc[df_latest[metric].idxmax()]\n",
    "    print(f\"\\n{metric.upper()}:\")\n",
    "    print(f\"  Model: {best_overall['model_name']}\")\n",
    "    print(f\"  Experiment: {best_overall['experiment_type']}\")\n",
    "    print(f\"  Score: {best_overall[metric]:.4f}\")\n",
    "    print(f\"  Timestamp: {best_overall['timestamp']}\")\n",
    "\n",
    "# Best overall model (average of all metrics)\n",
    "df_latest['avg_score'] = df_latest[numeric_cols].mean(axis=1)\n",
    "best_model = df_latest.loc[df_latest['avg_score'].idxmax()]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST OVERALL MODEL (Average of All Metrics)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model:      {best_model['model_name']}\")\n",
    "print(f\"Experiment: {best_model['experiment_type']}\")\n",
    "print(f\"Accuracy:   {best_model['accuracy']:.4f}\")\n",
    "print(f\"Precision:  {best_model['precision']:.4f}\")\n",
    "print(f\"Recall:     {best_model['recall']:.4f}\")\n",
    "print(f\"F1-Score:   {best_model['f1']:.4f}\")\n",
    "print(f\"Avg Score:  {best_model['avg_score']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0d7ce",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualization: Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35035386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    pivot_data = df_latest.pivot_table(\n",
    "        index='model_name',\n",
    "        columns='experiment_type',\n",
    "        values=metric\n",
    "    )\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    pivot_data.plot(kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_title(f'{title} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_ylabel(title, fontsize=12)\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.legend(title='Experiment', fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.3f', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Comparison chart saved as 'model_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5926bda",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Visualization: Radar Chart for Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "# Prepare data for radar chart\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Calculate angle for each axis\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Create separate radar charts for Preprocessed and Raw\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "for ax, exp_type in zip([ax1, ax2], ['Preprocessed', 'Raw']):\n",
    "    exp_data = df_latest[df_latest['experiment_type'] == exp_type]\n",
    "    \n",
    "    for _, row in exp_data.iterrows():\n",
    "        values = [row['accuracy'], row['precision'], row['recall'], row['f1']]\n",
    "        values += values[:1]\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=row['model_name'])\n",
    "        ax.fill(angles, values, alpha=0.15)\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, fontsize=11)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)\n",
    "    ax.set_title(f'{exp_type} Dataset', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('radar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Radar chart saved as 'radar_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f22b7",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Visualization: Heatmap of All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap showing all metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for idx, exp_type in enumerate(['Preprocessed', 'Raw']):\n",
    "    exp_data = df_latest[df_latest['experiment_type'] == exp_type]\n",
    "    heatmap_data = exp_data[['model_name'] + numeric_cols].set_index('model_name')\n",
    "    \n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        annot=True,\n",
    "        fmt='.4f',\n",
    "        cmap='YlGnBu',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        ax=axes[idx],\n",
    "        cbar_kws={'label': 'Score'}\n",
    "    )\n",
    "    axes[idx].set_title(f'{exp_type} Dataset - Performance Heatmap', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Metrics', fontsize=12)\n",
    "    axes[idx].set_ylabel('Models', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('heatmap_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Heatmap saved as 'heatmap_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccb1135",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary_report = df_latest[[\n",
    "    'model_name', 'experiment_type', 'accuracy', 'precision', 'recall', 'f1', 'loss', 'timestamp'\n",
    "]].sort_values(['model_name', 'experiment_type'])\n",
    "\n",
    "# Save to Excel with multiple sheets\n",
    "with pd.ExcelWriter('waste_classification_report.xlsx', engine='openpyxl') as writer:\n",
    "    # Sheet 1: Summary\n",
    "    summary_report.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    # Sheet 2: All Results\n",
    "    df.to_excel(writer, sheet_name='All Results', index=False)\n",
    "    \n",
    "    # Sheet 3: Statistics\n",
    "    summary_stats.to_excel(writer, sheet_name='Statistics')\n",
    "    \n",
    "    # Sheet 4: Comparison Table\n",
    "    comparison_table.to_excel(writer, sheet_name='Comparison')\n",
    "\n",
    "print(\"✓ Excel report saved as 'waste_classification_report.xlsx'\")\n",
    "\n",
    "# Save summary as CSV\n",
    "summary_report.to_csv('summary_report.csv', index=False)\n",
    "print(\"✓ Summary CSV saved as 'summary_report.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c550a3",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Generate Markdown Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b51f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown report\n",
    "markdown_report = f\"\"\"\n",
    "# Waste Classification Model Comparison Report\n",
    "\n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "**Project:** MMU TCV6313 Waste Classification (Plastic, Aluminum, Paper)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Executive Summary\n",
    "\n",
    "This report presents a comprehensive comparison of three deep learning models for waste classification:\n",
    "- **EfficientNetB2** (TensorFlow)\n",
    "- **ConvNeXt-Tiny** (PyTorch)\n",
    "- **ViT-B16** (PyTorch)\n",
    "\n",
    "Each model was evaluated on two datasets:\n",
    "- **Preprocessed Dataset**: Offline preprocessed images\n",
    "- **Raw Dataset**: Original unprocessed images\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Best Overall Model\n",
    "\n",
    "- **Model:** {best_model['model_name']}\n",
    "- **Experiment:** {best_model['experiment_type']}\n",
    "- **Accuracy:** {best_model['accuracy']:.4f}\n",
    "- **Precision:** {best_model['precision']:.4f}\n",
    "- **Recall:** {best_model['recall']:.4f}\n",
    "- **F1-Score:** {best_model['f1']:.4f}\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Results Summary\n",
    "\n",
    "{summary_report.to_markdown(index=False)}\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Key Findings\n",
    "\n",
    "### Preprocessed vs Raw Dataset\n",
    "\"\"\"\n",
    "\n",
    "for model in df_latest['model_name'].unique():\n",
    "    prep_data = df_latest[(df_latest['model_name'] == model) & (df_latest['experiment_type'] == 'Preprocessed')]\n",
    "    raw_data = df_latest[(df_latest['model_name'] == model) & (df_latest['experiment_type'] == 'Raw')]\n",
    "    \n",
    "    if not prep_data.empty and not raw_data.empty:\n",
    "        prep_acc = prep_data['accuracy'].values[0]\n",
    "        raw_acc = raw_data['accuracy'].values[0]\n",
    "        improvement = (prep_acc - raw_acc) * 100\n",
    "        \n",
    "        markdown_report += f\"\"\"\n",
    "**{model}:**\n",
    "- Preprocessed: {prep_acc:.4f}\n",
    "- Raw: {raw_acc:.4f}\n",
    "- Improvement: {improvement:+.2f}%\n",
    "\"\"\"\n",
    "\n",
    "markdown_report += \"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Recommendations\n",
    "\n",
    "Based on the evaluation results:\n",
    "\n",
    "1. **For Production Deployment:** Use the best overall model for optimal performance\n",
    "2. **For Resource-Constrained Environments:** Consider the trade-off between accuracy and model size\n",
    "3. **Data Preprocessing:** The results demonstrate the importance of proper preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Visualizations\n",
    "\n",
    "See the following generated charts:\n",
    "- `model_comparison.png` - Bar charts comparing all metrics\n",
    "- `radar_comparison.png` - Radar charts for visual comparison\n",
    "- `heatmap_comparison.png` - Heatmaps showing performance across all metrics\n",
    "\n",
    "---\n",
    "\n",
    "*Report generated automatically from training results*\n",
    "\"\"\"\n",
    "\n",
    "# Save markdown report\n",
    "with open('FINAL_REPORT.md', 'w') as f:\n",
    "    f.write(markdown_report)\n",
    "\n",
    "print(\"✓ Markdown report saved as 'FINAL_REPORT.md'\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL REPORTS GENERATED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - waste_classification_report.xlsx\")\n",
    "print(\"  - summary_report.csv\")\n",
    "print(\"  - FINAL_REPORT.md\")\n",
    "print(\"  - model_comparison.png\")\n",
    "print(\"  - radar_comparison.png\")\n",
    "print(\"  - heatmap_comparison.png\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
